{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.75\n",
      "True Positive 5\n",
      "True Negative 1\n",
      "False Positive 0\n",
      "False Negative 2\n",
      "Accuracy :- 0.75\n",
      "Precision value :- 1.0\n",
      "Recall score value :- 0.7142857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9d6340850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :- 0.8333333333333333\n",
      "Log loss metric value 0.4352001013091729\n",
      "Result of Log Loss metric 0.4352001013091729\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct_labels = 0\n",
    "    for true, pred in zip(y_true,y_pred):\n",
    "        if true == pred:\n",
    "            correct_labels += 1\n",
    "    return correct_labels / len(y_true)\n",
    "\n",
    "\n",
    "y_true = [1,0,1,1,1,0,0,1]  # Actual values of target column\n",
    "y_pred = [1,1,1,1,1,1,0,1]  # predicted values by model\n",
    "result = accuracy(y_true, y_pred)\n",
    "print(f\"Accuracy {result}\")\n",
    "\n",
    "# ===========================================\n",
    "\n",
    "# Accuracy with Scikit learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_accuracy = accuracy_score(y_true, y_pred)\n",
    "# print(\"Accuracy with Scikit-learn:: {}\".format(model_accuracy))\n",
    "\n",
    "\n",
    "# Calculating individual components of confusion matrix\n",
    "def true_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input :- y_true - list of actual values\n",
    "        y_pred - list of predicted values\n",
    "    Output :- number of true positives\n",
    "    \"\"\"\n",
    "    tp_counts = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true==1 and pred==1:\n",
    "            tp_counts += 1\n",
    "    return tp_counts\n",
    "\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input :- y_true - list of actual values\n",
    "            y_pred - list of predicted values\n",
    "    Output :- number of true negatives\n",
    "    \"\"\"\n",
    "    tn_counts = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true==0 and pred==0:\n",
    "            tn_counts += 1\n",
    "    return tn_counts\n",
    "\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input :- y_true - list of actual values\n",
    "            y_pred - list of predicted values\n",
    "    Output :- number of false positives\n",
    "    \"\"\"\n",
    "    fp_counts = 0\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        if true==1 and pred==0:\n",
    "            fp_counts += 1\n",
    "    return fp_counts\n",
    "\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input :- y_true - list of actual values\n",
    "            y_pred - list of predicted values\n",
    "    Output :- number of false negatives\n",
    "    \"\"\"\n",
    "    fn_counts = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true==0 and pred==1:\n",
    "            fn_counts += 1\n",
    "    return fn_counts\n",
    "\n",
    "\n",
    "def get_confusion_matrix_components(y_ture, y_pred):\n",
    "\n",
    "    \"\"\"\n",
    "    Input :- y_true - list of actual values\n",
    "            y_pred - list of predicted values\n",
    "    Output :- None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tp = true_positive(y_true, y_pred)  # Calculating ture positive\n",
    "    print(f\"True Positive {tp}\")\n",
    "\n",
    "    tn = true_negative(y_true, y_pred)  # Calculating ture negative\n",
    "    print(f\"True Negative {tn}\")\n",
    "\n",
    "    fp = false_positive(y_true, y_pred) # Calculating false positive\n",
    "    print(f\"False Positive {fp}\")\n",
    "\n",
    "    fn = false_negative(y_true, y_pred) # Calculating false negative\n",
    "    print(f\"False Negative {fn}\")\n",
    "\n",
    "\n",
    "get_confusion_matrix_components(y_true, y_pred)\n",
    "\n",
    "# Calcuating accuracy score with confusion matrix components\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input :- y_true :- list of actual values\n",
    "            y_pred :- list of predicted values\n",
    "    Output:- float value of accuracy score\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    accuracy_score = (tp+tn)/(tp+tn+fp+fn)\n",
    "    return accuracy_score\n",
    "\n",
    "\n",
    "acc_using_terms = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy :- {acc_using_terms}\")\n",
    "\n",
    "# Calculating precision\n",
    "def precision_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input :- y_true :- list of actual values\n",
    "            y_pred :- list of predicted values\n",
    "    Output:- float value of precision score\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "\n",
    "    precision_value = tp/(tp+fp)\n",
    "    return precision_value\n",
    "\n",
    "precision_value = precision_score(y_true, y_pred)\n",
    "print(f\"Precision value :- {precision_value}\")\n",
    "\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input :- y_true :- list of actual values\n",
    "            y_pred :- list of predicted values\n",
    "    Output:- float value of precision score\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    recall_value = tp/(tp+fn)\n",
    "    return recall_value\n",
    "\n",
    "\n",
    "recall_value = recall_score(y_true, y_pred)\n",
    "print(f\"Recall score value :- {recall_value}\")\n",
    "\n",
    "\n",
    "# Precision recall curves\n",
    "import matplotlib.pyplot as plt\n",
    "def precision_recall_treshholds(y_true, y_pred):\n",
    "    # how we assumed these thresholds is a long story\n",
    "    precisions = []\n",
    "    recall = []\n",
    "  thresholds = [0.0490937, 0.05934905, 0.079377,  0.08584789, 0.11114267,\n",
    "  0.11639273,  0.15952202, 0.17554844, 0.18521942,  0.27259048, 0.31620708,\n",
    "  0.33056815,  0.39095342, 0.61977213]\n",
    "  for i in thresholds:\n",
    "    temp_prediction = [1 if x >= i else 0 for x in y_pred]\n",
    "    p = precision_score(y_true, temp_prediction)\n",
    "    r = recall_score(y_true, temp_prediction)\n",
    "    precisions.append(p)\n",
    "    recall.append(r)\n",
    "\n",
    "  return precisions, recall\n",
    "\n",
    "y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "y_pred = [0.02638412, 0.11114267, 0.31620708, 0.0490937, 0.0191491, 0.17554844,\n",
    "0.15952202, 0.03819563, 0.11639273,0.079377, 0.08584789, 0.39095342,0.27259048, 0.03447096, 0.04644807, 0.03543574, 0.18521942, 0.05934905, 0.61977213, 0.33056815]\n",
    "\n",
    "precision_values, recall_values = precision_recall_treshholds(y_true, y_pred)\n",
    "\n",
    "# based on precision and recall values we can draw precision recall curve\n",
    "plt.title('precision-recall curve')\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.plot(precision_values, recall_values, color='blue', linewidth=3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calcuating F1 score\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input :- y_true :- list of actual values\n",
    "            y_pred :- list of predicted values\n",
    "    Output:- float value of f1_score\n",
    "    \"\"\"\n",
    "    p = precision_score(y_true, y_pred)\n",
    "    r = recall_score(y_true, y_pred)\n",
    "\n",
    "    f1_score_value = 2 * p * r /(p+r)\n",
    "    return f1_score_value\n",
    "\n",
    "# Actual values of target column\n",
    "y_true = [1,0,1,1,1,0,0,1]\n",
    "# predicted values by model\n",
    "y_pred = [1,1,1,1,1,1,0,1]\n",
    "\n",
    "f1_score_value = f1_score(y_true, y_pred)\n",
    "print(f\"F1 score :- {f1_score_value}\")\n",
    "\n",
    "\n",
    "# Calculating logloss\n",
    "import numpy as np\n",
    "def log_loss(y_true, y_probabilities):\n",
    "    #this value is used to clip probabilities\n",
    "    epsilon = 1e-15\n",
    "    losses = []\n",
    "\n",
    "    for yt, yp in list(zip(y_true, y_probabilities)):\n",
    "\n",
    "        temp_yp = np.clip(yp, epsilon, 1 - epsilon)\n",
    "        # calculate loss\n",
    "        temp_loss = - 1.0 * (yt*np.log(temp_yp) + (1-yt)*np.log(1- temp_yp))\n",
    "        losses.append(temp_loss)\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "y_probabilities = [0.02638412, 0.11114267, 0.31620708, 0.0490937, 0.0191491,\n",
    " 0.17554844, 0.15952202, 0.03819563, 0.11639273,0.079377, 0.08584789, 0.39095342,\n",
    " 0.27259048, 0.03447096, 0.04644807, 0.03543574, 0.18521942, 0.05934905, 0.61977213,\n",
    " 0.33056815]\n",
    "\n",
    "log_loss = log_loss(y_true, y_probabilities)\n",
    "print(f\"Log loss metric value {log_loss}\")\n",
    "\n",
    "## Logloss function with sklearn\n",
    "from sklearn import metrics\n",
    "def log_loss_metric(y_true, y_probabilities):\n",
    "    return metrics.log_loss(y_true, y_probabilities)\n",
    "\n",
    "log_loss = log_loss_metric(y_true, y_probabilities)\n",
    "print(f\"Result of Log Loss metric {log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
